{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": "<img src=\"https://github.com/Microsoft/sqlworkshops/blob/master/graphics/solutions-microsoft-logo-small.png?raw=true\" alt=\"Microsoft\">\r\n<br>\r\n\r\n# SQL Server 2019 big data cluster Tutorial\r\n## 00 - Scenario Overview and System Setup\r\n\r\nIn this set of tutorials you'll work with an end-to-end scenario that uses SQL Server 2019's big data clusters to solve real-world problems. \r\n",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Wide World Importers\r\n\r\n**TODO:** Describe company and use-case",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Running these Tutorials\r\n\r\n- You can read through the output of these completed tutorials if you wish - or:\r\n\r\n- You can follow along with the steps you see in these tutorials by copying the code into a SQL Query window and Spark Notebook using the Azure Data Studio tool, or you can click here to download these Jupyter Notebooks and run them in Azure Data Studio for a hands-on experience.\r\n \r\n- If you would like to run the tutorials, you'll need a SQL Server 2019 big data cluster and the client tools installed. If you want to set up your own cluster, <a href=\"https://docs.microsoft.com/en-us/sql/big-data-cluster/deploy-get-started?view=sqlallproducts-allversions\" target=\"_blank\">click this reference and follow the steps you see there for the server and tools you need</a>.\r\n\r\n- You will need to have the following: \r\n    - Your **Knox Password**\r\n    - The **Knox IP Address**\r\n    - The `sa` **Username** and **Password** to your Master Instance\r\n    - The **IP address** to the SQL Server big data cluster Master Instance \r\n    - The **name** of your big data cluster\r\n\r\nFor a complete workshop on SQL Server 2019's big data clusters, <a href=\"https://github.com/Microsoft/sqlworkshops/tree/master/sqlserver2019bigdataclusters\" target=\"_blank\">check out this resource</a>.",
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": "## Copy Database backups to the SQL Server 2019 big data cluster Master Instance\r\n\r\n**TODO:** describe curl, etc. Code for file transfer, describe restore process",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "!md d:\\temp\r\n\r\n!curl -G \"https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/WWI.bak\" -o d:\\temp\\WWI.bak\r\n!curl -G \"https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/WWIDW.bak\" -o d:\\temp\\WWIDW.bak\r\n\r\n!kubectl cp d:/temp/WWI.bak mssql-master-pool-0:/var/opt/mssql/data -c mssql-server -n <ReplaceWithClusterName>\r\n!kubectl cp d:/temp/WWIDW.bak mssql-master-pool-0:/var/opt/mssql/data -c mssql-server -n <ReplaceWithClusterName>",
            "metadata": {},
            "outputs": [],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": "## Copy Exported Data to Storage Pool\r\n\r\n**TODO:** Explain curls to hdfs",
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": "!curl -i -L -k -u root:<ReplacewithKnoxPassword> -X PUT \"https://<ReplaceWithKnoxEndpoint>:30443/gateway/default/webhdfs/v1/product_review_data?op=MKDIRS\"\r\n!curl -i -L -k -u root:<ReplacewithKnoxPassword> -X PUT \"https://<ReplaceWithKnoxEndpoint>:30443/gateway/default/webhdfs/v1/adventureworks_export?op=MKDIRS\"\r\n\r\n!curl -G \"https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/product_reviews.csv\" -o d:\\temp\\product_reviews.csv\r\n!curl -G \"https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/customers.csv\" -o d:\\temp\\customers.csv\r\n!curl -G \"https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/web_clickstreams.csv\" -o d:\\temp\\web_clickstreams.csv\r\n!curl -G \"https://cs7a9736a9346a1x44c6xb00.blob.core.windows.net/backups/stockitemholdings.csv\" -o d:\\temp\\stockitemholdings.csv\r\n\r\n!curl -i -L -k -u root:<ReplacewithKnoxPassword> -X PUT \"https://<ReplaceWithKnoxEndpoint>:30443/gateway/default/webhdfs/v1/product_review_data/product_reviews.csv?op=create&overwrite=true\" -H \"Content-Type: application/octet-stream\" -T \"product_reviews.csv\"\r\n!curl -i -L -k -u root:<ReplacewithKnoxPassword> -X PUT \"https://<ReplaceWithKnoxEndpoint>:30443/gateway/default/webhdfs/v1/adventureworks_export/customers.csv?op=create&overwrite=true\" -H \"Content-Type: application/octet-stream\" -T \"customers.csv.csv\"\r\n!curl -i -L -k -u root:<ReplacewithKnoxPassword> -X PUT \"https://<ReplaceWithKnoxEndpoint>:30443/gateway/default/webhdfs/v1/adventureworks_export/web_clickstreams.csv?op=create&overwrite=true\" -H \"Content-Type: application/octet-stream\" -T \"web_clickstreams.csv\"\r\n!curl -i -L -k -u root:<ReplacewithKnoxPassword> -X PUT \"https://<ReplaceWithKnoxEndpoint>:30443/gateway/default/webhdfs/v1/adventureworks_export/stockitemholdings.csv?op=create&overwrite=true\" -H \"Content-Type: application/octet-stream\" -T \"stockitemholdings.csv\"",
            "metadata": {},
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": "## Next Step: Working with the SQL Server 2019 big data cluster Master Instance\r\n\r\n**TODO:** Add Link",
            "metadata": {}
        }
    ]
}